{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "We'll use the following libraries in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, time\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) \n",
      "[GCC 7.2.0]\n",
      "Pandas Version: 0.22.0\n",
      "Numpy Version: 1.13.3\n",
      "Seaborn Version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Pandas Version:\", pd.__version__)\n",
    "print(\"Numpy Version:\", np.__version__)\n",
    "print(\"Seaborn Version:\", sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Getting the right data \n",
    "We pulled in weekly turnstile data from the MTA portal: http://web.mta.info/developers/turnstile.html\n",
    "\n",
    "First, we create a list of the weeks we're interested in fetching data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['170701',\n",
       " '170624',\n",
       " '170617',\n",
       " '170610',\n",
       " '170603',\n",
       " '170527',\n",
       " '170520',\n",
       " '170513',\n",
       " '170506',\n",
       " '170429',\n",
       " '170422',\n",
       " '170415',\n",
       " '170408',\n",
       " '170401',\n",
       " '160702',\n",
       " '160625',\n",
       " '160618',\n",
       " '160611',\n",
       " '160604',\n",
       " '160528',\n",
       " '160521',\n",
       " '160514',\n",
       " '160507',\n",
       " '160430',\n",
       " '160423',\n",
       " '160416',\n",
       " '160409',\n",
       " '160402']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define list of weeks we want to pull from the MTA portal\n",
    "\n",
    "def datelist(startdate):\n",
    "    \"\"\"\n",
    "    For a given Saturday, make a list of dates for the 14 previous Saturdays\n",
    "    \"\"\"\n",
    "    week_list = [startdate + ((timedelta(days=-7))*i) for i in range(14)]\n",
    "    clean_weeks = [i.strftime('%y%m%d') for i in week_list]\n",
    "    return clean_weeks\n",
    "\n",
    "\n",
    "# Define the last Saturday we're interested in for 2016 and 2017\n",
    "start17 = datetime(2017, 7, 1)\n",
    "start16 = datetime(2016, 7, 2)\n",
    "\n",
    "# We'll import data for the 14 weeks preceeding July 1st for both 2016 and 2017\n",
    "weeks_to_import = datelist(start17) + datelist(start16)\n",
    "weeks_to_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iterate through our list of dates to pull weekly files from the MTA portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadturndata(date):\n",
    "    # Build the filename\n",
    "    strdate = str(date)\n",
    "    filename = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_'+strdate+'.txt'\n",
    "\n",
    "    # Read in the csv\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "\n",
    "def loadturnlist(dates):\n",
    "    \"\"\"\n",
    "    We'll iterarte through the list of weeks to create dataframes using loadturndata and then concat together into one dataframe \n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    x = []\n",
    "    for i in dates:\n",
    "        df = (loadturndata(i))\n",
    "        x.append(df)\n",
    "    data = pd.concat(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This takes a few minutes to run - go treat yourself to a cup of tea!\n",
    "raw = loadturnlist(weeks_to_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle the raw data in case things go south in the cleaning process and you need to start over from here\n",
    "raw.to_pickle('data/raw_turnstile_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "All the fun stuff (jk!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns and add datetime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below to read in the pickled raw data if you are starting here\n",
    "raw = pd.read_pickle('data/raw_turnstile_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df = raw.rename(columns=lambda x: x.strip().lower())\n",
    "\n",
    "# Concat date and time and convert to datetime object\n",
    "df['datetime'] = df['date'] + ' ' + df['time']\n",
    "df['datetime_clean'] = [datetime.strptime(x, '%m/%d/%Y %H:%M:%S') for x in df['datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add some helpful date-part columns\n",
    "df['year'] = [x.year for x in df['datetime_clean']]\n",
    "df['weekday'] = df[['datetime_clean']].apply(lambda x: datetime.strftime(x['datetime_clean'], '%A'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find delta counts for distinct turnstiles at given time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create group ID for distinct turnstiles\n",
    "df['group'] = df['c/a'].astype(str) + \\\n",
    "                df['unit'].astype(str) + \\\n",
    "                df['scp'].astype(str) + \\\n",
    "                df['station'].astype(str)  + \\\n",
    "                df['linename'].astype(str) + \\\n",
    "                df['division'].astype(str) + \\\n",
    "                df['year'].astype(str)\n",
    "                \n",
    "# Map 'group' string to integer id     \n",
    "groups = set(df['group'])\n",
    "\n",
    "\n",
    "def groups_dict(groups):\n",
    "    group_dict = defaultdict(int)\n",
    "    for i in enumerate(list(groups)):\n",
    "        group_dict[i[1]]= i[0]\n",
    "\n",
    "    return group_dict\n",
    "\n",
    "group_id_dict = groups_dict(groups)\n",
    "\n",
    "df['group_id'] = [group_id_dict[x] for x in df['group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create station ID for later grouping on distinct stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create station ID for distinct stations\n",
    "df['station_line'] = df['station'].astype(str) + \\\n",
    "                df['linename'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort values in dataframe by group id and datatime to find diff in counts from prev row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort values by group id and date to find diff in turnstile counts from prev row\n",
    "df.sort_values(['group_id','datetime_clean'], inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "def find_diff_prev_row(df_series_col):\n",
    "    col_array = np.array(df_series_col)\n",
    "    col_array_shifted = shift(col_array, 1, cval=np.NaN)\n",
    "    col_diff = abs(col_array - col_array_shifted)\n",
    "\n",
    "    return col_diff\n",
    "\n",
    "\n",
    "df['entries_diff'] = find_diff_prev_row(df['entries'])\n",
    "df['exit_diff'] = find_diff_prev_row(df['exits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set invalid diff values to nan (first row of turnstile partitions and negative values from reboots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify first rows for each group partition to use as mask when setting invalid values to nan\n",
    "def find_first_rows_groups(df_series_col):\n",
    "    col_array = np.array(df_series_col)\n",
    "    col_array_shifted = shift(col_array, 1, cval=np.NaN)\n",
    "    first_row_mask = col_array != col_array_shifted\n",
    "\n",
    "    return first_row_mask\n",
    "\n",
    "\n",
    "df['first_row_group'] = find_first_rows_groups(df['group_id'])\n",
    "\n",
    "# Make entries_diff and exit_diff nan when first row in group or negative value\n",
    "df.loc[df['first_row_group'], 'entries_diff'] = None\n",
    "df.loc[df['entries_diff'] < 0, 'entries_diff'] = None\n",
    "\n",
    "df.loc[df['first_row_group'], 'exit_diff'] = None\n",
    "df.loc[df['exit_diff'] < 0, 'exit_diff'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deal with outliers and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call the describe method to check out the distribution of data for the entry and exit diffs calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Outliers are commonly defined as values that fall above 1.5 IQR from the 75th Q. If we use this definition, ~13% of our values would qualify as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_outliers(df_series, multiple_IQR):\n",
    "    \"\"\"\n",
    "    For a series of numerical values, remove the zeros and identify the upper outliers \n",
    "    to return a mask for all outliers in series\n",
    "    \"\"\"\n",
    "    non_zeros = df_series.replace(0, None)\n",
    "    \n",
    "    adjusted_IQR = (non_zeros.quantile(.75) - non_zeros.quantile(.25)) * multiple_IQR\n",
    "    outlier_lim = non_zeros.quantile(.75) + adjusted_IQR\n",
    "    print(outlier_lim)\n",
    "    \n",
    "    outliers = [True if x > outlier_lim else False for x in df_series]\n",
    "    \n",
    "    outlier_count = sum(outliers)\n",
    "    all_data_count = len(df_series)\n",
    "    print('{} outliers identified: {} of all data'.format(outlier_count, round(outlier_count/all_data_count,6)))\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Entries Outliers')\n",
    "df['entries_outlier'] = find_outliers(df['entries_diff'], 5)\n",
    "\n",
    "print('\\n Exit Outliers')\n",
    "df['exit_outlier'] = find_outliers(df['exit_diff'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All Data Len:', len(df))\n",
    "\n",
    "clean_df = df.loc[(~df['entries_outlier'])].copy()\n",
    "print('Excluding Outliers Len:', len(clean_df))\n",
    "\n",
    "print('Keeping', round(len(clean_df)/len(df), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "We've decided to remove null values from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null entry diffs', clean_df.entries_diff.isnull().sum())\n",
    "print('Null exit diffs', clean_df.exit_diff.isnull().sum())\n",
    "print('Clean Data len:', len(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.dropna(subset = ['entries_diff', 'exit_diff'], how='any', inplace=True)\n",
    "\n",
    "print('Null entry diffs', clean_df.entries_diff.isnull().sum())\n",
    "print('Null exit diffs', clean_df.exit_diff.isnull().sum())\n",
    "print('Clean Data len:', len(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrown_away = len(df) - len(clean_df)\n",
    "print(\"We're throwing away {} data points - about {} of the total\".format(thrown_away, round(thrown_away/len(df), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add columns for aggregating & exploring distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['week'] = [x.isocalendar()[1] for x in clean_df['datetime_clean']]## Find average daily entry volume by station\n",
    "clean_df['hour'] = [x.hour for x in clean_df['datetime_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to bin the times in 4 hour increments and rename the hour groups and weekday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timebin(hour):\n",
    "    if hour ==0:\n",
    "        return 6\n",
    "    if hour <= 4:\n",
    "        return 1\n",
    "    if hour <=8:\n",
    "        return 2\n",
    "    if hour <=12:\n",
    "        return 3\n",
    "    if hour <= 16:\n",
    "        return 4\n",
    "    if hour <= 20:\n",
    "        return 5\n",
    "    if hour <= 24:\n",
    "        return 6\n",
    "    \n",
    "hourgroups = {6:'8pm - 12am', \n",
    "              1: '12am - 4am', \n",
    "              2:'4am - 8am', \n",
    "              3:'8am - 12pm', \n",
    "              4:'12pm - 4pm', \n",
    "              5:'4pm - 8pm'}\n",
    "\n",
    "wkdaynbr = {'Friday': 5,\n",
    " 'Monday': 1,\n",
    " 'Saturday': 6,\n",
    " 'Sunday': 0,\n",
    " 'Thursday': 4,\n",
    " 'Tuesday': 2,\n",
    " 'Wednesday': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['timegroup'] = clean_df['hour'].apply(timebin)\n",
    "clean_df['timegroupstr'] = clean_df['timegroup'].map(hourgroups)\n",
    "clean_df['wkdaynbr'] = clean_df['weekday'].map(wkdaynbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find daily average entry traffic for each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picked the cleaned turnstile dataset for joining with station data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df.to_pickle('data/cleaned_turnstile_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find average daily entry volume by station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find daily average entries per station\n",
    "stations_day = clean_df.groupby(['station_line', 'date']).sum()\n",
    "stations_day.reset_index(inplace=True)\n",
    "\n",
    "daily_avg = stations_day.groupby('station_line')['entries_diff'].mean()\n",
    "daily_avg.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(daily_avg, hist=True, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_avg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
