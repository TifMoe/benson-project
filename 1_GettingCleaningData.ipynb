{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "We'll use the following libraries in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, time\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) \n",
      "[GCC 7.2.0]\n",
      "Pandas Version: 0.22.0\n",
      "Numpy Version: 1.13.3\n",
      "Seaborn Version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Pandas Version:\", pd.__version__)\n",
    "print(\"Numpy Version:\", np.__version__)\n",
    "print(\"Seaborn Version:\", sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Getting the right data \n",
    "We pulled in weekly turnstile data from the MTA portal: http://web.mta.info/developers/turnstile.html\n",
    "\n",
    "First, we create a list of the weeks we're interested in fetching data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['170701',\n",
       " '170624',\n",
       " '170617',\n",
       " '170610',\n",
       " '170603',\n",
       " '170527',\n",
       " '170520',\n",
       " '170513',\n",
       " '170506',\n",
       " '170429',\n",
       " '170422',\n",
       " '170415',\n",
       " '170408',\n",
       " '170401',\n",
       " '160702',\n",
       " '160625',\n",
       " '160618',\n",
       " '160611',\n",
       " '160604',\n",
       " '160528',\n",
       " '160521',\n",
       " '160514',\n",
       " '160507',\n",
       " '160430',\n",
       " '160423',\n",
       " '160416',\n",
       " '160409',\n",
       " '160402']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define list of weeks we want to pull from the MTA portal\n",
    "\n",
    "def datelist(startdate):\n",
    "    \"\"\"\n",
    "    For a given Saturday, make a list of dates for the 14 previous Saturdays\n",
    "    \"\"\"\n",
    "    week_list = [startdate + ((timedelta(days=-7))*i) for i in range(14)]\n",
    "    clean_weeks = [i.strftime('%y%m%d') for i in week_list]\n",
    "    return clean_weeks\n",
    "\n",
    "\n",
    "# Define the last Saturday we're interested in for 2016 and 2017\n",
    "start17 = datetime(2017, 7, 1)\n",
    "start16 = datetime(2016, 7, 2)\n",
    "\n",
    "# We'll import data for the 14 weeks preceeding July 1st for both 2016 and 2017\n",
    "weeks_to_import = datelist(start17) + datelist(start16)\n",
    "weeks_to_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iterate through our list of dates to pull weekly files from the MTA portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadturndata(date):\n",
    "    # Build the filename\n",
    "    strdate = str(date)\n",
    "    filename = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_'+strdate+'.txt'\n",
    "\n",
    "    # Read in the csv\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "\n",
    "def loadturnlist(dates):\n",
    "    \"\"\"\n",
    "    We'll iterarte through the list of weeks to create dataframes using loadturndata and then concat together into one dataframe \n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    x = []\n",
    "    for i in dates:\n",
    "        df = (loadturndata(i))\n",
    "        x.append(df)\n",
    "    data = pd.concat(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2f4e699969a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note: This takes a few minutes to run - go treat yourself to a cup of tea!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadturnlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweeks_to_import\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ab495be4a4eb>\u001b[0m in \u001b[0;36mloadturnlist\u001b[0;34m(dates)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloadturndata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ab495be4a4eb>\u001b[0m in \u001b[0;36mloadturndata\u001b[0;34m(date)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Read in the csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[0;32m--> 433\u001b[0;31m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readall_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Note: This takes a few minutes to run - go treat yourself to a cup of tea!\n",
    "raw = loadturnlist(weeks_to_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle the raw data in case things go south in the cleaning process and you need to start over from here\n",
    "raw.to_pickle('data/raw_turnstile_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "All the fun stuff (jk!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns and add datetime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below to read in the pickled raw data if you are starting here\n",
    "raw = pd.read_pickle('data/raw_turnstile_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df = raw.rename(columns=lambda x: x.strip().lower())\n",
    "\n",
    "# Concat date and time and convert to datetime object\n",
    "df['datetime'] = df['date'] + ' ' + df['time']\n",
    "df['datetime_clean'] = [datetime.strptime(x, '%m/%d/%Y %H:%M:%S') for x in df['datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add some helpful date-part columns\n",
    "df['year'] = [x.year for x in df['datetime_clean']]\n",
    "df['weekday'] = df[['datetime_clean']].apply(lambda x: datetime.strftime(x['datetime_clean'], '%A'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c/a</th>\n",
       "      <th>unit</th>\n",
       "      <th>scp</th>\n",
       "      <th>station</th>\n",
       "      <th>linename</th>\n",
       "      <th>division</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>desc</th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime_clean</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/24/2017</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6233682</td>\n",
       "      <td>2110437</td>\n",
       "      <td>06/24/2017 00:00:00</td>\n",
       "      <td>2017-06-24 00:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/24/2017</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6233696</td>\n",
       "      <td>2110445</td>\n",
       "      <td>06/24/2017 04:00:00</td>\n",
       "      <td>2017-06-24 04:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/24/2017</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6233712</td>\n",
       "      <td>2110473</td>\n",
       "      <td>06/24/2017 08:00:00</td>\n",
       "      <td>2017-06-24 08:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/24/2017</td>\n",
       "      <td>12:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6233790</td>\n",
       "      <td>2110560</td>\n",
       "      <td>06/24/2017 12:00:00</td>\n",
       "      <td>2017-06-24 12:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/24/2017</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>6233942</td>\n",
       "      <td>2110622</td>\n",
       "      <td>06/24/2017 16:00:00</td>\n",
       "      <td>2017-06-24 16:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    c/a  unit       scp station linename division        date      time  \\\n",
       "0  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/24/2017  00:00:00   \n",
       "1  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/24/2017  04:00:00   \n",
       "2  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/24/2017  08:00:00   \n",
       "3  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/24/2017  12:00:00   \n",
       "4  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/24/2017  16:00:00   \n",
       "\n",
       "      desc  entries    exits             datetime      datetime_clean  year  \\\n",
       "0  REGULAR  6233682  2110437  06/24/2017 00:00:00 2017-06-24 00:00:00  2017   \n",
       "1  REGULAR  6233696  2110445  06/24/2017 04:00:00 2017-06-24 04:00:00  2017   \n",
       "2  REGULAR  6233712  2110473  06/24/2017 08:00:00 2017-06-24 08:00:00  2017   \n",
       "3  REGULAR  6233790  2110560  06/24/2017 12:00:00 2017-06-24 12:00:00  2017   \n",
       "4  REGULAR  6233942  2110622  06/24/2017 16:00:00 2017-06-24 16:00:00  2017   \n",
       "\n",
       "    weekday  \n",
       "0  Saturday  \n",
       "1  Saturday  \n",
       "2  Saturday  \n",
       "3  Saturday  \n",
       "4  Saturday  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find delta counts for distinct turnstiles at given time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create group ID for distinct turnstiles\n",
    "df['group'] = df['c/a'].astype(str) + \\\n",
    "                df['unit'].astype(str) + \\\n",
    "                df['scp'].astype(str) + \\\n",
    "                df['station'].astype(str)  + \\\n",
    "                df['linename'].astype(str) + \\\n",
    "                df['division'].astype(str) + \\\n",
    "                df['year'].astype(str)\n",
    "                \n",
    "# Map 'group' string to integer id     \n",
    "groups = set(df['group'])\n",
    "\n",
    "\n",
    "def groups_dict(groups):\n",
    "    group_dict = defaultdict(int)\n",
    "    for i in enumerate(list(groups)):\n",
    "        group_dict[i[1]]= i[0]\n",
    "\n",
    "    return group_dict\n",
    "\n",
    "group_id_dict = groups_dict(groups)\n",
    "\n",
    "df['group_id'] = [group_id_dict[x] for x in df['group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create station ID for later grouping on distinct stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create station ID for distinct stations\n",
    "df['station_line'] = df['station'].astype(str) + \\\n",
    "                df['linename'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort values in dataframe by group id and datatime to find diff in counts from prev row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort values by group id and date to find diff in turnstile counts from prev row\n",
    "df.sort_values(['group_id','datetime_clean'], inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "def find_diff_prev_row(df_series_col):\n",
    "    col_array = np.array(df_series_col)\n",
    "    col_array_shifted = shift(col_array, 1, cval=np.NaN)\n",
    "    col_diff = abs(col_array - col_array_shifted)\n",
    "\n",
    "    return col_diff\n",
    "\n",
    "\n",
    "df['entries_diff'] = find_diff_prev_row(df['entries'])\n",
    "df['exit_diff'] = find_diff_prev_row(df['exits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set invalid diff values to nan (first row of turnstile partitions and negative values from reboots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify first rows for each group partition to use as mask when setting invalid values to nan\n",
    "def find_first_rows_groups(df_series_col):\n",
    "    col_array = np.array(df_series_col)\n",
    "    col_array_shifted = shift(col_array, 1, cval=np.NaN)\n",
    "    first_row_mask = col_array != col_array_shifted\n",
    "\n",
    "    return first_row_mask\n",
    "\n",
    "\n",
    "df['first_row_group'] = find_first_rows_groups(df['group_id'])\n",
    "\n",
    "# Make entries_diff and exit_diff nan when first row in group or negative value\n",
    "df.loc[df['first_row_group'], 'entries_diff'] = None\n",
    "df.loc[df['entries_diff'] < 0, 'entries_diff'] = None\n",
    "\n",
    "df.loc[df['first_row_group'], 'exit_diff'] = None\n",
    "df.loc[df['exit_diff'] < 0, 'exit_diff'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deal with outliers and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call the describe method to check out the distribution of data for the entry and exit diffs calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>year</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entries_diff</th>\n",
       "      <th>exit_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.481151e+06</td>\n",
       "      <td>5.481151e+06</td>\n",
       "      <td>5.481151e+06</td>\n",
       "      <td>5.481151e+06</td>\n",
       "      <td>5.471787e+06</td>\n",
       "      <td>5.471787e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.668874e+07</td>\n",
       "      <td>2.942573e+07</td>\n",
       "      <td>2.016504e+03</td>\n",
       "      <td>4.681123e+03</td>\n",
       "      <td>1.098745e+04</td>\n",
       "      <td>1.020665e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.995975e+08</td>\n",
       "      <td>1.789708e+08</td>\n",
       "      <td>4.999825e-01</td>\n",
       "      <td>2.701979e+03</td>\n",
       "      <td>3.712465e+06</td>\n",
       "      <td>3.654583e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.016000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.403665e+05</td>\n",
       "      <td>2.665350e+05</td>\n",
       "      <td>2.016000e+03</td>\n",
       "      <td>2.344000e+03</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.597810e+06</td>\n",
       "      <td>1.490638e+06</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>4.680000e+03</td>\n",
       "      <td>8.200000e+01</td>\n",
       "      <td>5.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.628317e+06</td>\n",
       "      <td>4.688856e+06</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>7.019000e+03</td>\n",
       "      <td>2.580000e+02</td>\n",
       "      <td>1.730000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147483e+09</td>\n",
       "      <td>2.097170e+09</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>9.363000e+03</td>\n",
       "      <td>2.130766e+09</td>\n",
       "      <td>2.097170e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            entries         exits          year      group_id  entries_diff  \\\n",
       "count  5.481151e+06  5.481151e+06  5.481151e+06  5.481151e+06  5.471787e+06   \n",
       "mean   3.668874e+07  2.942573e+07  2.016504e+03  4.681123e+03  1.098745e+04   \n",
       "std    1.995975e+08  1.789708e+08  4.999825e-01  2.701979e+03  3.712465e+06   \n",
       "min    0.000000e+00  0.000000e+00  2.016000e+03  0.000000e+00  0.000000e+00   \n",
       "25%    5.403665e+05  2.665350e+05  2.016000e+03  2.344000e+03  1.100000e+01   \n",
       "50%    2.597810e+06  1.490638e+06  2.017000e+03  4.680000e+03  8.200000e+01   \n",
       "75%    6.628317e+06  4.688856e+06  2.017000e+03  7.019000e+03  2.580000e+02   \n",
       "max    2.147483e+09  2.097170e+09  2.017000e+03  9.363000e+03  2.130766e+09   \n",
       "\n",
       "          exit_diff  \n",
       "count  5.471787e+06  \n",
       "mean   1.020665e+04  \n",
       "std    3.654583e+06  \n",
       "min    0.000000e+00  \n",
       "25%    9.000000e+00  \n",
       "50%    5.500000e+01  \n",
       "75%    1.730000e+02  \n",
       "max    2.097170e+09  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Outliers are commonly defined as values that fall above 1.5 IQR from the 75th Q. If we use this definition, ~13% of our values would qualify as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_outliers(df_series, multiple_IQR):\n",
    "    \"\"\"\n",
    "    For a series of numerical values, remove the zeros and identify the upper outliers \n",
    "    to return a mask for all outliers in series\n",
    "    \"\"\"\n",
    "    non_zeros = df_series.replace(0, None)\n",
    "    \n",
    "    adjusted_IQR = (non_zeros.quantile(.75) - non_zeros.quantile(.25)) * multiple_IQR\n",
    "    outlier_lim = non_zeros.quantile(.75) + adjusted_IQR\n",
    "    print(outlier_lim)\n",
    "    \n",
    "    outliers = [True if x > outlier_lim else False for x in df_series]\n",
    "    \n",
    "    outlier_count = sum(outliers)\n",
    "    all_data_count = len(df_series)\n",
    "    print('{} outliers identified: {} of all data'.format(outlier_count, round(outlier_count/all_data_count,6)))\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries Outliers\n",
      "1564.0\n",
      "21642 outliers identified: 0.003948 of all data\n",
      "\n",
      " Exit Outliers\n",
      "1041.0\n",
      "69320 outliers identified: 0.012647 of all data\n"
     ]
    }
   ],
   "source": [
    "print('Entries Outliers')\n",
    "df['entries_outlier'] = find_outliers(df['entries_diff'], 5)\n",
    "\n",
    "print('\\n Exit Outliers')\n",
    "df['exit_outlier'] = find_outliers(df['exit_diff'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data Len: 5481151\n",
      "Excluding Outliers Len: 5459509\n",
      "Keeping 0.996052\n"
     ]
    }
   ],
   "source": [
    "print('All Data Len:', len(df))\n",
    "\n",
    "clean_df = df.loc[(~df['entries_outlier'])].copy()\n",
    "print('Excluding Outliers Len:', len(clean_df))\n",
    "\n",
    "print('Keeping', round(len(clean_df)/len(df), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "We've decided to remove null values from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null entry diffs 9364\n",
      "Null exit diffs 9364\n",
      "Clean Data len: 5459509\n"
     ]
    }
   ],
   "source": [
    "print('Null entry diffs', clean_df.entries_diff.isnull().sum())\n",
    "print('Null exit diffs', clean_df.exit_diff.isnull().sum())\n",
    "print('Clean Data len:', len(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null entry diffs 0\n",
      "Null exit diffs 0\n",
      "Clean Data len: 5450145\n"
     ]
    }
   ],
   "source": [
    "clean_df.dropna(subset = ['entries_diff', 'exit_diff'], how='any', inplace=True)\n",
    "\n",
    "print('Null entry diffs', clean_df.entries_diff.isnull().sum())\n",
    "print('Null exit diffs', clean_df.exit_diff.isnull().sum())\n",
    "print('Clean Data len:', len(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're throwing away 31006 data points - about 0.0057 of the total\n"
     ]
    }
   ],
   "source": [
    "thrown_away = len(df) - len(clean_df)\n",
    "print(\"We're throwing away {} data points - about {} of the total\".format(thrown_away, round(thrown_away/len(df), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add columns for aggregating & exploring distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['week'] = [x.isocalendar()[1] for x in clean_df['datetime_clean']]## Find average daily entry volume by station\n",
    "clean_df['hour'] = [x.hour for x in clean_df['datetime_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to bin the times in 4 hour increments and rename the hour groups and weekday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timebin(hour):\n",
    "    if hour ==0:\n",
    "        return 6\n",
    "    if hour <= 4:\n",
    "        return 1\n",
    "    if hour <=8:\n",
    "        return 2\n",
    "    if hour <=12:\n",
    "        return 3\n",
    "    if hour <= 16:\n",
    "        return 4\n",
    "    if hour <= 20:\n",
    "        return 5\n",
    "    if hour <= 24:\n",
    "        return 6\n",
    "    \n",
    "hourgroups = {6:'8pm - 12am', \n",
    "              1: '12am - 4am', \n",
    "              2:'4am - 8am', \n",
    "              3:'8am - 12pm', \n",
    "              4:'12pm - 4pm', \n",
    "              5:'4pm - 8pm'}\n",
    "\n",
    "wkdaynbr = {'Friday': 5,\n",
    " 'Monday': 1,\n",
    " 'Saturday': 6,\n",
    " 'Sunday': 0,\n",
    " 'Thursday': 4,\n",
    " 'Tuesday': 2,\n",
    " 'Wednesday': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['timegroup'] = clean_df['hour'].apply(timebin)\n",
    "clean_df['timegroupstr'] = clean_df['timegroup'].map(hourgroups)\n",
    "clean_df['wkdaynbr'] = clean_df['weekday'].map(wkdaynbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c/a</th>\n",
       "      <th>unit</th>\n",
       "      <th>scp</th>\n",
       "      <th>station</th>\n",
       "      <th>linename</th>\n",
       "      <th>division</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>desc</th>\n",
       "      <th>entries</th>\n",
       "      <th>...</th>\n",
       "      <th>entries_diff</th>\n",
       "      <th>exit_diff</th>\n",
       "      <th>first_row_group</th>\n",
       "      <th>entries_outlier</th>\n",
       "      <th>exit_outlier</th>\n",
       "      <th>week</th>\n",
       "      <th>hour</th>\n",
       "      <th>timegroup</th>\n",
       "      <th>timegroupstr</th>\n",
       "      <th>wkdaynbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97614</th>\n",
       "      <td>N523</td>\n",
       "      <td>R300</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>2 AV</td>\n",
       "      <td>F</td>\n",
       "      <td>IND</td>\n",
       "      <td>03/25/2017</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>16253671</td>\n",
       "      <td>...</td>\n",
       "      <td>251.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12am - 4am</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97615</th>\n",
       "      <td>N523</td>\n",
       "      <td>R300</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>2 AV</td>\n",
       "      <td>F</td>\n",
       "      <td>IND</td>\n",
       "      <td>03/25/2017</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>16253760</td>\n",
       "      <td>...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4am - 8am</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97616</th>\n",
       "      <td>N523</td>\n",
       "      <td>R300</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>2 AV</td>\n",
       "      <td>F</td>\n",
       "      <td>IND</td>\n",
       "      <td>03/25/2017</td>\n",
       "      <td>12:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>16254010</td>\n",
       "      <td>...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8am - 12pm</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97617</th>\n",
       "      <td>N523</td>\n",
       "      <td>R300</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>2 AV</td>\n",
       "      <td>F</td>\n",
       "      <td>IND</td>\n",
       "      <td>03/25/2017</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>16254463</td>\n",
       "      <td>...</td>\n",
       "      <td>453.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>12pm - 4pm</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97618</th>\n",
       "      <td>N523</td>\n",
       "      <td>R300</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>2 AV</td>\n",
       "      <td>F</td>\n",
       "      <td>IND</td>\n",
       "      <td>03/25/2017</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>16254958</td>\n",
       "      <td>...</td>\n",
       "      <td>495.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>4pm - 8pm</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        c/a  unit       scp station linename division        date      time  \\\n",
       "97614  N523  R300  00-00-00    2 AV        F      IND  03/25/2017  04:00:00   \n",
       "97615  N523  R300  00-00-00    2 AV        F      IND  03/25/2017  08:00:00   \n",
       "97616  N523  R300  00-00-00    2 AV        F      IND  03/25/2017  12:00:00   \n",
       "97617  N523  R300  00-00-00    2 AV        F      IND  03/25/2017  16:00:00   \n",
       "97618  N523  R300  00-00-00    2 AV        F      IND  03/25/2017  20:00:00   \n",
       "\n",
       "          desc   entries    ...     entries_diff exit_diff first_row_group  \\\n",
       "97614  REGULAR  16253671    ...            251.0     151.0           False   \n",
       "97615  REGULAR  16253760    ...             89.0     134.0           False   \n",
       "97616  REGULAR  16254010    ...            250.0     492.0           False   \n",
       "97617  REGULAR  16254463    ...            453.0     787.0           False   \n",
       "97618  REGULAR  16254958    ...            495.0    1192.0           False   \n",
       "\n",
       "       entries_outlier exit_outlier week  hour timegroup  timegroupstr  \\\n",
       "97614            False        False   12     4         1    12am - 4am   \n",
       "97615            False        False   12     8         2     4am - 8am   \n",
       "97616            False        False   12    12         3    8am - 12pm   \n",
       "97617            False        False   12    16         4    12pm - 4pm   \n",
       "97618            False         True   12    20         5     4pm - 8pm   \n",
       "\n",
       "       wkdaynbr  \n",
       "97614         6  \n",
       "97615         6  \n",
       "97616         6  \n",
       "97617         6  \n",
       "97618         6  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find daily average entry traffic for each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picked the cleaned turnstile dataset for joining with station data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df.to_pickle('data/cleaned_turnstile_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find average daily entry volume by station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find daily average entries per station\n",
    "stations_day = clean_df.groupby(['station_line', 'date']).sum()\n",
    "stations_day.reset_index(inplace=True)\n",
    "\n",
    "daily_avg = stations_day.groupby('station_line')['entries_diff'].mean()\n",
    "daily_avg.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(daily_avg, hist=True, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_avg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
